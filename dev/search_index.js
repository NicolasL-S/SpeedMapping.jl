var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The following benchmarks compare various SpeedMapping specifications with similar packages. Given the great diversity of problems, the only criterion for convergence is always |residual| < abstol, even when different algorithms may converge to different fixed points, zeros, or minima. abstol is set to 1e-7, except for a few especially long or difficult problems. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Every effort has been made to ensure a fair comparison of the packages. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The same algorithms are run with the same parameters (e.g. the maximum number of lags for Anderson Acceleration is set to 30). \nOtherwise, default options are preferred to get \"out-of-the-box\" performances. \nInitialization time is included in compute time. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The benchmarking scripts, raw data, and detailed results are available here.","category":"page"},{"location":"benchmarks/#Accelerating-convergent-mapping-iterations","page":"Benchmarks","title":"Accelerating convergent mapping iterations","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"For mapping applications, AA has various options available:","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"with or without adaptive relaxation\nwithout composition, composite with :aa1, composite with :acx2. \nFor mapping applications with objective function available, monotonicity (with tolerance for objective deterioration) can also be imposed or not.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"totalling 2 × 3 × 2 = 12 possible specifications. For ACX, the default options are used.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Julia has two other commonly used packages for fixed-point acceleration: FixedPoint and FixedPointAcceleration (with algorithms SEA, VEA, MPE, RRE and Anderson).","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The benchmarks are based on 15 problems from physics, statistics, genetics and social sciences from the FixedPointTestProblems library.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Mapping results)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Marker indicates how much longer each algorithm took relative to the fastest algorithm for each problem. The color scale shows the same for the number of maps. The left-most algorithms are the most reliable and the quickest. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"ACX and monotonic AA perform well (non-monotonic AA did not converge for the Lange, ancestry problem). While AA tends to need fewer iterations, ACX is lighter and tends to be quicker for small-scale problems. Composite AA is also surprisingly fast, with :aa1 having a slight edge on :acx2. With few exceptions, adaptative relaxation is also advantageous, especially for large scale problems like Bratu, the Poisson equation (electric field) and the lid-driven cavity flow.","category":"page"},{"location":"benchmarks/#Solving-non-linear-systems-of-equations","page":"Benchmarks","title":"Solving non-linear systems of equations","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"SciML has already benchmarked nonlinear solvers extensively using the library of 23 challenging problems. A simpler version of these tests is done here for NonlinearSolve and NLSolve (with default choice of AD backend). In SpeedMapping, only standard AA is available for solving non-linear equations.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Problems)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Each marker's height shows how much longer each algorithm took relative to the fastest for each problem. The color scale shows the same for the number of iterations.SpeedMapping's AA and NonlinearSolve's Newton Raphston and Polyalgorithm are generally reliable and fast.  NLSolve trust_region and newton are also surprisingly fast, despite their higher number of function evaluations. A caveat to these tests is that all problems have 10 variables or less. Adding lager-scale problems with various sparsity patterns would paint a more complete picture.","category":"page"},{"location":"benchmarks/#Minimizing-a-function","page":"Benchmarks","title":"Minimizing a function","text":"","category":"section"},{"location":"benchmarks/#Without-constraint","page":"Benchmarks","title":"Without constraint","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"ACX shines for problems where the gradient is available and does not rely on the Hessian matrix. A natural comparison is thus with Optim's L-BFGS and conjugate gradient algorithms. To avoid relying on autodiff or libraries external to Julia, a good test set is the unconstrained test problems from ArtificialLandscapes, most of which come from the CUTEst test suite. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Performance, Optim)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The previous graph shows the fraction of problems solved within a factor pi of the time taken by the fastest solver. When no algorithm converged in time for a problem, it is removed from the list. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The most reliable algorithm is obviously LBFGS which converged reasonably fast for nearly all problems while ACX and the Conjugate Gradient converged for approximately 80% of them. While ACX can struggle for poorly conditioned problems (which abound in CUTEst), it was the fastest 60% of the time, making it potentially useful for reasonably well-conditioned problems, especially with manually-coded gradients.","category":"page"},{"location":"benchmarks/#With-box-constraints","page":"Benchmarks","title":"With box constraints","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"There are no constrained problems in ArtificialLandscapes, but we can easily add arbitrary ones for testing purposes. For each problem with starting point x0 an upper bound x0 .+ 0.5 is imposed, and no lower bound. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"An advantage of ACX is that box constraints add little extra computation. Optim adds lower and upper bounds using barrier functions via fminbox. Another interesting option is LBFGSB.jl, a wrapper around a Fortran implementation of L-BFGS-B, which handles constraints differently. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"A problem is considered having converged when the |gradientᵢ| < 1e-7 for all i for which no constraint was binding. A constraint is considered binding if |xᵢ - boundᵢ| < 1e-7 and gradientᵢ < 0 (since it is a minimization with upper bound).","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Performance, Optim, constraint)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Among the four algorithms tested, there seems to be no reason not to use ACX. It would be interesting to test other bound-constrained solvers like tron from JSOSolvers.jl.","category":"page"},{"location":"api/#Notation","page":"Interface","title":"Notation","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"Problems:","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"MAP: Accelerating convergent mapping iterations\nNLS: Solving a non-linear systems of equations\nMIN: Minimizing a function, possibly with box-constraints","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"Algorithms:","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"ACX: Alternating cyclic extrapolations\nAA: Anderson Acceleration","category":"page"},{"location":"api/#speedmapping","page":"Interface","title":"speedmapping","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"speedmapping(x0; kwargs...) :: SpeedMappingResult","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"x0 :: T is the starting point and defines the type:","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"For ACX: x0 can be of type Real or Complex with mutable or immutable containers of different shapes like AbstractArray, StaticArray, Real, or Tuple.\nFor AA: x0 should be a mutable AbstractArray{AbstractFloat}.","category":"page"},{"location":"api/#Keyword-arguments-defining-the-problem","page":"Interface","title":"Keyword arguments defining the problem","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"One and only one of the following argument should be supplied. They are all of type FN = Union{Function, Nothing}.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"m! :: FN = nothing in-place mapping function for MAP with mutable arrays as input. ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"speedmapping([1.,1.]; m! = (xout, xin) -> xout .=  0.9xin)","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"r! :: FN = nothing in-place residual function for NLS with mutable arrays as input. ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"speedmapping([1.,1.]; r! = (resid, x) -> resid .=  -0.1x)","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"g! :: FN = nothing in-place gradient function for MIN with mutable arrays as input. ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"speedmapping([1.,1.]; g! = (grad, x) -> grad .=  4x.^3)","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"m and g are versions of m! and g! with immutable types like Real, StaticArray, or Tuple as input and output.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"using StaticArrays\nspeedmapping(1.; m = x -> 0.9x)\nspeedmapping(SA[1.,1.]; m = x -> 0.9x)\nspeedmapping(1.; g = x -> 4x^3)\nspeedmapping((1.,1.); g = x -> (x[1] - 2, x[2]^3))\n","category":"page"},{"location":"api/#Other-important-keyword-arguments","page":"Interface","title":"Other important keyword arguments","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"algo :: Symbol = r! !==  nothing  ?  :aa  :  :acx determines the method used.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":":acx can be used to solve MAP or MIN (the default for MAP).\n:aa can be used to solve MAP or NLS. ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"f :: FN = nothing computes an objective function. ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"For MIN, f is be used to initialize the learning rate better.\nFor MAP using AA, f is be used ensure monotonicity of the algorithm. \nFor NLS, f is ignored.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"lower, upper = nothing define bounds on parameters which can be used with any problem.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"speedmapping([1.,1.]; g!  =  (grad, x)  -> grad .= 4x.^3, lower = [-Inf,2.])","category":"page"},{"location":"api/#Other-keyword-arguments","page":"Interface","title":"Other keyword arguments","text":"","category":"section"},{"location":"api/#Affecting-both-**ACX**-and-**AA**","page":"Interface","title":"Affecting both ACX and AA","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"cache ::  Union{AcxCache, AaCache, Nothing}  =  nothing   pre-allocates memory for ACX or AA with mutable input","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"c = AaCache([1.,1.])\nspeedmapping([1.,1.]; m! = (xout, xin) -> xout .=  0.9xin, algo = :aa, cache = c)","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"abstol :: AbstractFloat = 1e-8   The absolute tolerance used as stopping criterion. ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"For MAP, the algorithm stops when ‖xout - xin‖ < abstol (from m!(xout, xin))\nFor NLS, the algorithm stops when ‖res‖ < abstol (from r!(res, x))\nFor MIN, the algorithm stops when ‖gradient‖ < abstol (from g!(gradient, x))","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"pnorm :: Real = 2.   The norm used for the stopping criterion. Typically 1, 2 or Inf.  ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"maps_limit :: Real = 1_000_000_000   The number of main function evaluation (m!, r!, or g!) before the algorithm terminates.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"iter_limit :: Real = 1_000_000_000     The number of iterations before the algorithm terminates.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"time_limit :: Real = Inf   The time limit before stopping (if time_limit == Inf, time() will not be called at each iteration).","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"reltol_resid_grow :: Real = algo == :aa ? 4. : (g! !== nothing || g !== nothing) ? 1e5 : 100.   reltol_resid_grow is a problem-specific stabilizing parameter. After a mapping/descent step/iteration, the distance between the current x and the previous x is reduced until the residual norm (‖xout - xin‖, ‖res‖, or ‖grad‖) does not increase more than a by a factor reltol_resid_grow. It is set to 4 for AA because this algorithm is more sensitive to low-quality iterations and because NLS may involve highly divergent functions. For ACX it is set to 100 for MAP, and for 1e5 for MIN.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"buffer :: AbstractFloat  = (m! !==  nothing  || m !==  nothing) ?  0.05  :  0.   bufferis used in conjunction with lower or upper. If an iterate x lands outside a constraint, buffer leaves some distance between x and the constraint. It is set by default to 0.05 for MAP because constraints may be used to avoid landing immediately on bad values like saddle points at which the algorithm would stall. ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"store_trace :: Bool = false   To store information on each iteration of the solving process. The trace depends on the algorithm. For a SpeedMapping result res, the trace is ","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"res.acx_trace for ACX;\nres.aa_trace for AA.","category":"page"},{"location":"api/#Affecting-only-**ACX**","page":"Interface","title":"Affecting only ACX","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"orders = (2,3,3)   The extrapolation orders. (2,3,3) is extremely reliable, but others like (2,3), or (2,) could be considered.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"initial_learning_rate ::  Real  =  1.   The initial learning rate used for MIN. If initialize_learning_rate == true, it is the starting point for the initialization.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"initialize_learning_rate :: Bool = true   To find a suitable learning rate to start MIN for which the residual norm does not increase too fast and the change in the objective (if supplied) respects the Armijo condition.","category":"page"},{"location":"api/#Affecting-only-**AA**","page":"Interface","title":"Affecting only AA","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"lags ::  Integer  =  30   The maximum number of past residuals used to compute the next iterate","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"condition_max :: Real = 1e6   The maximum condition number of the matrix of past residuals used to compute the next iterate. Setting it too high increases the risk of numerical imprecision.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"relax_default :: Real = 1.   The default relaxation parameter (also referred to as damping or mixing parameter).","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"ada_relax :: Symbol = m! !== nothing ? :minimum_distance : :none   Adaptive relaxation. For now, only :minimum_distance is implemented (see Lepage-Saucier, 2024 although changes were made to the regularization). It is set to :none for NLS since :minimum_distance requires convergent mapping to be useful.","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"composite :: Symbol = :none   Composite Anderson Acceleration by Chen and Vuik, 2022. A one-step AA iteration (using 2 maps) is inserted between 2 full AA steps, which reduces  the computation and can offer interesting speed-up for some applications. Two types are implemented: :aa1 and acx2 (which inserts an ACX, order 2 step).","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"abstol_obj_grow ::  Real  =  √abstol    If f is supplied,  AA monitors the growth of the objective. ","category":"page"},{"location":"api/#SpeedMappingResult","page":"Interface","title":"SpeedMappingResult","text":"","category":"section"},{"location":"api/","page":"Interface","title":"Interface","text":"SpeedMappingResult has fields","category":"page"},{"location":"api/","page":"Interface","title":"Interface","text":"minimizer :: typeof(x0): The solution\nresidual_norm :: AbstractFloat: The norm of the residual, which would be ‖xout - xin‖ for MAP, ‖residual‖ for NLS, and ‖∇f(x)‖ for MIN (only for non-binding components of the gradient).\nmaps: the number of maps, function evaluations or gradient evaluations\nf_calls: The number of objective function evaluations\niterations: The number of iteration\nstatus :: Symbol ∈ (:first_order, :max_iter, :max_eval, :max_time, :failure) should be :first_order if a solution has been found successfully.\nalgo ∈ (:acx, :aa)\nacx_trace A vector of AcxState if algo == :acx && store_trace == true, nothing otherwise.\naa_trace A vector of AaState if algo == :aa && store_trace == true, nothing otherwise.\nlast_learning_rate :: AbstractFloat The last learning rate, only meaningful for MIN.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"SpeedMapping","category":"page"},{"location":"#SpeedMapping","page":"Introduction","title":"SpeedMapping","text":"SpeedMapping\n\n(Image: Build Status) (Image: codecov) (Image: Stable)\n\nSpeedMapping solves three types of problems:\n\nAccelerating convergent mapping iterations\nSolving non-linear systems of equations\nMinimizing a function, possibly with box constraints\n\nusing two algorithms: Alternating cyclic extrapolations and Anderson Acceleration. With recently developed features, SpeedMapping is competitive with similar Julia packages.\n\n\n\n\n\n","category":"module"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"EditURL = \"tutorial.jl\"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping(x0; kwargs...)  solves three types of problems:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Accelerating convergent mapping iterations\nSolving non-linear systems of equations\nMinimizing a function, possibly with box constraints","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using two algorithms:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Alternating cyclic extrapolations (ACX) Lepage-Saucier, 2024\nAnderson Acceleration (AA) Anderson, 1964","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This tutorial will display its main functionality on simple problems. To see which specification may be more performant for your problem, the Benchmarks section compares all of them, along with other Julia packages with similar functionalities.","category":"page"},{"location":"tutorial/#Accelerating-convergent-mapping-iterations","page":"Tutorial","title":"Accelerating convergent mapping iterations","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's find the dominant eigenvalue of a matrix A using the accelerated Power iteration.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra\n\nn = 10;\nA = ones(n,n) .+ Diagonal(1:n);\n\n# An in-place mapping function to avoid allocations\nfunction power_iteration!(xout, xin, A)\n    mul!(xout, A, xin)\n    maxabs = 0.\n    for xi in xout\n        abs(xi) > maxabs && (maxabs = abs(xi))\n    end\n    xout ./= maxabs\nend;\nx0 = ones(n);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Speedmapping has one mandatory argument: the starting point x0. The mapping is specified with the keyword argument m!.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using SpeedMapping\nres = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A));\ndisplay(res)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The dominant eigenvalue is:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"v = res.minimizer; # The dominant eigenvector\ndominant_eigenvalue = v'A*v/v'v;\neigen(A).values[10] ≈ dominant_eigenvalue","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With m!, the default algorithm is algo = :acx. To switch, set algo = :aa.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res_aa = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa);\ndisplay(res_aa)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default, AA uses adaptive relaxation, which can reduce the number of iterations. It is specified by the keyword argument ada_relax = :minimum_distance. For constant relaxation, set ada_relax = :none.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa, ada_relax = :none);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Another recent development for AA is Composite AA by Chen and Vuik, 2022. A one-step AA iteration (using 2 maps) is inserted between 2 full AA steps, which reduces the computation and can speed-up some applications. The default is composite = :none. Two versions are available:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa, composite = :aa1);\nres = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa, composite = :acx2);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Some mapping iterations maximize or minimize a certain objective function. Since some AA steps can deteriorate the objective, it would be best to avoid them by falling back to the last map. This can be done by supplying an objective function (assumed to be a minimization problem) using f as keyword argument. Here is an illustrative EM-algorithm example from Hasselblad (1969) estimating a mixture of exponential distributions.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function neg_log_likelihood(x)\n    freq = (162, 267, 271, 185, 111, 61, 27, 8, 3, 1)\n    p, μ1, μ2 = x\n    yfact = μ1expy = μ2expy = 1\n    log_lik = 0\n    for y in eachindex(freq)\n        log_lik += freq[y] * log((p * exp(-μ1) * μ1expy + (1 - p) * exp(-μ2) * μ2expy) / yfact)\n        yfact *= y\n        μ1expy *= μ1\n        μ2expy *= μ2\n    end\n    return -log_lik # Negative log likelihood to get a minimization problem\nend\n\nfunction EM_map!(xout, xin)\n    freq = (162, 267, 271, 185, 111, 61, 27, 8, 3, 1)\n    p, μ1, μ2 = xin\n    sum_freq_z1 = sum_freq_z2 = sum_freq_y_z1 = sum_freq_y_z2 = 0\n    μ1expy = μ2expy = 1\n    for i in eachindex(freq)\n        z = p * exp(-μ1) * μ1expy / (p * exp(-μ1) * μ1expy + (1 - p) * exp(-μ2) * μ2expy)\n        sum_freq_z1   += freq[i] * z\n        sum_freq_z2   += freq[i] * (1 - z)\n        sum_freq_y_z1 += (i - 1) * freq[i] * z\n        sum_freq_y_z2 += (i - 1) * freq[i] * (1 - z)\n        μ1expy *= μ1\n        μ2expy *= μ2\n    end\n    xout .= (sum_freq_z1 / sum(freq), sum_freq_y_z1 / sum_freq_z1, sum_freq_y_z2 / sum_freq_z2)\nend\n\nres_with_objective = speedmapping([0.25, 1., 2.]; f = neg_log_likelihood, m! = EM_map!, algo = :aa);\ndisplay(res_with_objective)","category":"page"},{"location":"tutorial/#Simple-constraints-on-parameters","page":"Tutorial","title":"Simple constraints on parameters","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In the previous example, the parameters being estimated, p μ₁ μ₂, have restricted domains; 1  p  1 is a probability (where the degenerate values p = 0 and p = 1 are hopefully avoided), and μ₁ μ₂  0 are the inverse scales of exponential distributions. To avoid the risk that an iterate falls outside of its domain, bounds can be supplied using the keyword arguments lower and upper:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res_with_objective = speedmapping([0.25, 1., 2.]; f = neg_log_likelihood, m! = EM_map!, algo = :aa,\n    lower = [0, 0, 0], upper = [1., Inf, Inf], buffer = 0.05);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here, the keyword argument buffer (= 0.05 by default for mapping applications) ensures that a parameter xᵢ will be at least at a distance buffer  xᵢprev - boundᵢ of boundᵢ, where xᵢprev is xᵢ's previous value. This safeguard avoids jumping to boundᵢ instantly (unless buffer = 0), in case boundᵢ is infeasible or is a saddle point. For instance, if xᵢprev = 02, lowerᵢ = 0, but AA's unconstrained next iterate would be xᵢtry = -01, then xᵢ is set to max(xᵢtry, buffer  xᵢprev + (1 - buffer)  lowerᵢ) = max(-01 005  02 + 095  0) = 001.","category":"page"},{"location":"tutorial/#Avoiding-memory-allocation","page":"Tutorial","title":"Avoiding memory allocation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For similar problems solved many times, it is possible to preallocate working memory and feed it using the cache keyword argument. Each algorithm has its own cache:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"acx_cache = AcxCache(x0);\naa_cache = AaCache(x0);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that x0 must still be supplied to speedmapping.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For small-sized problems with ACX, heap allocation can be avoided by supplying a static array as starting point and using the keyword argument m for the mapping function:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using StaticArrays\n\nfunction power_iteration(xin, A)\n    xout = A * xin\n    maxabs = 0.\n    for xi in xout\n        abs(xi) > maxabs && (maxabs = abs(xi))\n    end\n    return xout / maxabs;\nend\n\nAs = @SMatrix ones(n,n);\nAs += Diagonal(1:n);\nx0s = @SVector ones(n);\n\nres_static = speedmapping(x0s; m = x -> power_iteration(x, As));\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Comparing speed gains","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using BenchmarkTools, Unitful\n\nbench_eigen = @benchmark eigen($A);\nbench_alloc = @benchmark speedmapping($x0; m! = (xout, xin) -> power_iteration!(xout, xin, $A));\nbench_prealloc = @benchmark speedmapping($x0; m! = (xout, xin) -> power_iteration!(xout, xin, $A), cache = $acx_cache);\nbench_nonalloc = @benchmark speedmapping($x0s; m = x -> power_iteration(x, $As));\ntimes = Int.(round.(median.([bench_eigen.times, bench_alloc.times, bench_prealloc.times, bench_nonalloc.times])))/1000 .* u\"μs\";\nreturn hcat([\"eigen\", \"Allocating\", \"Pre-allocated\", \"Non allocating\"],times)","category":"page"},{"location":"tutorial/#Working-with-immutable-types","page":"Tutorial","title":"Working with immutable types","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Along with StaticArray, m accepts other immutable types like Real' andTuple`.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping(0.5; m = cos);\nspeedmapping((0.5, 0.5); m = x -> (cos(x[1]), sin(x[2])));\nnothing #hide","category":"page"},{"location":"tutorial/#Solving-non-linear-systems-of-equations","page":"Tutorial","title":"Solving non-linear systems of equations","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For non-linear systems of equations (finding x^* such that G(x^*) = 0), only AA with constant relaxation should be used (and is set by default). The keyword argument to supply G is r!.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function r!(resid, x)\n\tresid[1] = x[1]^2;\n\tresid[2] = (x[2] + x[1])^3;\nend\n\nres_nl = speedmapping([1.,2.]; r! = r!);\ndisplay(res_nl)","category":"page"},{"location":"tutorial/#Minimizing-a-function","page":"Tutorial","title":"Minimizing a function","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To minimize a function (using ACX), the function and its in-place gradient are supplied with the keyword arguments f and g!. The Hessian cannot be supplied. Compared to other quasi-Newton algorithms like L-BFGS, ACX iterations are very fast, but the algorithm may struggle for ill-conditioned problems.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"f_Rosenbrock(x) = 100 * (x[1]^2 - x[2])^2 + (x[1] - 1.)^2;\n\nfunction g_Rosenbrock!(grad, x) # Rosenbrock gradient\n\tgrad[1] = 400 * (x[1]^2 - x[2]) * x[1] + 2 * (x[1] - 1);\n\tgrad[2] = -200 * (x[1]^2 - x[2]);\nend\n\nres_optim = speedmapping([-1.2, 1.]; f = f_Rosenbrock, g! = g_Rosenbrock!);\ndisplay(res_optim)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The function objective is only used to compute a safer initial learning rate. It can be omitted.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping([-1.2, 1.]; g! = g_Rosenbrock!);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If only the objective is supplied, the gradient is computed using using ForwardDiff.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping([-1.2, 1.]; f = f_Rosenbrock);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The keyword argument g can be used with static arrays or tuple to supply a non-allocating gradient.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using StaticArrays\ng_Rosenbrock(x :: StaticArray) = SA[400 * (x[1]^2 - x[2]) * x[1] + 2 * (x[1] - 1), -200 * (x[1]^2 - x[2])];\nspeedmapping(SA[-1.2, 1.]; g = g_Rosenbrock);\n\ng_Rosenbrock(x :: Tuple) = (400 * (x[1]^2 - x[2]) * x[1] + 2 * (x[1] - 1), -200 * (x[1]^2 - x[2]));\nspeedmapping((-1.2, 1.); g = g_Rosenbrock);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Scalar functions can also be supplied. E.g. f(x) = e^x + x^2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res_scalar = speedmapping(0.; f = x -> exp(x) + x^2, g = x -> exp(x) + 2x);\ndisplay(res_scalar)","category":"page"},{"location":"tutorial/#Adding-box-constraints","page":"Tutorial","title":"Adding box constraints","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"An advantage of ACX is that constraints on parameters have little impact on estimation speed. They are added via the keyword arguments lower and upper (= nothing by default). The starting point does not need to be in the feasible domain.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping([-1.2, 1.]; f = f_Rosenbrock, g! = g_Rosenbrock!, lower = [2., -Inf]);\nspeedmapping(0.; g = x -> exp(x) + 2x, upper = -1);\nnothing #hide","category":"page"}]
}
