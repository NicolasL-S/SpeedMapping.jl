var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The following benchmarks compare various SpeedMapping specifications with other packages with similar functionalities. Given the great diversity of problems, the only criterion for convergence is always |residual| < abstol, even when different algorithms may converge to different fixed points or minima. abstol is set to 1e-7, except for a few especially long or difficult problems. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Every effort has been made to ensure a fair comparison of the packages. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The same algorithms are run with the same parameters (e.g. the maximum number of lags for Anderson Acceleration is set to 30). \nOtherwise, default options are preferred to get \"out-of-the-box\" performances. \nInitialization time is included in compute time. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The benchmarking scripts are available here.","category":"page"},{"location":"benchmarks/#Accelerating-convergent-mapping-iterations","page":"Benchmarks","title":"Accelerating convergent mapping iterations","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"For mapping applications, the ACX algorithm can be compared with AA:","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"with or without adaptive relaxation\nwith or without composition. \nFor mapping applications with objective function available, monotonicity (with tolerance for ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"objective deterioration) can also be imposed or not.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The two other most used fixed-point acceleration packages in Julia are FixedPoint and FixedPointAcceleration (with algorithms SEA, VEA, MPE, RRE and Anderson).","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The benchmarks are based on 15 problems from physics, statistics, genetics and social sciences from the FixedPointTestProblems library. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Mapping results)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The height of each marker indicates how much longer each algorithm took relative to the fastest algorithm for each problem. The color scale shows the same for the number of iterations. The left-most algorithms are the most reliable and the quickest. ACX and monotonic AA perform well. While AA tends to need fewer iterations, ACX is lighter and tends to be quicker.","category":"page"},{"location":"benchmarks/#Solving-non-linear-systems-of-equations","page":"Benchmarks","title":"Solving non-linear systems of equations","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"SciML has already performed extensive benchmarking of nonlinear solvers using the library of 23 challenging problems. A simpler version of these tests is done here for NonlinearSolve and NLSolve (with default choice of AD backend). In SpeedMapping, only standard AA is available for solving non-linear equations.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Problems)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Each marker's height shows how much longer each algorithm took relative to the fastest for each problem. The color scale shows the same for the number of iterations.SpeedMapping is generally reliable and fast. NLSolve trust_region and newton are also surprisingly fast, despite their higher number of function evaluations. A caveat to these tests is that all problems have 10 variables or less. Adding lager-scale problems whould paint a more complete picture.","category":"page"},{"location":"benchmarks/#Minimizing-a-function","page":"Benchmarks","title":"Minimizing a function","text":"","category":"section"},{"location":"benchmarks/#Without-constraint","page":"Benchmarks","title":"Without constraint","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"ACX shines for problems where the gradient is available and does not rely on the Hessian matrix. A natural comparison is thus with Optim's L-BFGS and conjugate gradient algorithms. To avoid relying on autodiff or libraries external to Julia, a good test set is the unconstrained test problems from ArtificialLandscapes, most of which come from the CUTEst test suite. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Performance, Optim)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The previous graph shows the fraction of problems solved within a factor pi of the time taken by the fastest solver. When no algorithm converged in time for a problem, it is removed from the list. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The most reliable algorithm is obviously LBFGS which converged to minima reasonably fast for nearly all the problems. ACX and Conjugate Gradient converged for approximately 80% of them. While ACX can struggle for poorly conditioned problems (which abound in CUTEst), it was the fastest 60% of the time, making it potentially useful for regular well-conditioned problems, especially with manually-coded gradients.","category":"page"},{"location":"benchmarks/#With-box-constraints","page":"Benchmarks","title":"With box constraints","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"There are no constrained problems in ArtificialLandscapes, but we can easily add arbitrary ones for testing purposes. For each problem with starting point x0 an upper bound x0 .+ 0.5 is imposed, and no lower bound. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"An advantage of ACX is that box constraints add little extra computation. Optim adds lower and upper bounds using barrier functions via fminbox. Another interesting option is LBFGSB.jl, a wrapper around a Fortran implementation of L-BFGS-B, which handles constraints differently. ","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"A problem is considered having converged when the |gradientᵢ| < 1e-7 for all i for which a constraint was non-binding. A constraint is considered binding if |xᵢ - boundᵢ| < 1e-7 and gradientᵢ < 0 (since it is a minimization with upper bound).","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Performance, Optim, constraint)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Among the four algorithms tested, there seems to be no reason not to use ACX. It would be interesting to test other bound-constrained solvers like tron from JSOSolvers.jl.","category":"page"},{"location":"api/","page":"API","title":"API","text":"This API will refer to 1, 2 and 3 to identify the problems:","category":"page"},{"location":"api/","page":"API","title":"API","text":"Accelerating convergent mapping iterations;\nSolving a non-linear systems of equations;\nMinimizing a function without constraint or with box-constraints.","category":"page"},{"location":"api/","page":"API","title":"API","text":"and will refer to Alternating cyclic extrapolations as ACX and Anderson Acceleration as AA.","category":"page"},{"location":"api/","page":"API","title":"API","text":"speedmapping(x0; kwargs...) :: SpeedMappingResult","category":"page"},{"location":"api/","page":"API","title":"API","text":"x0 :: T is the starting point and defines the type:","category":"page"},{"location":"api/","page":"API","title":"API","text":"For ACX: x0 can be of type Real or Complex with mutable or immutable containers of different shapes like AbstractArray, StaticArray, Scalar, Tuple.\nFor AA: x0 should be a mutable AbstractArray{AbstractFloat}.","category":"page"},{"location":"api/#Keyword-arguments-defining-the-problem","page":"API","title":"Keyword arguments defining the problem","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"One and only one of the following argument should be supplied. They are all of type FN = Union{Function, Nothing}.","category":"page"},{"location":"api/","page":"API","title":"API","text":"m! ::  FN  =  nothing in-place mapping function for problem 1 with mutable arrays as input. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"speedmapping([1.,1.]; m! = (xout, xin) -> xout .=  0.9xin)","category":"page"},{"location":"api/","page":"API","title":"API","text":"r! ::  FN  =  nothing in-place residual function for 2 with mutable arrays as input. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"speedmapping([1.,1.]; r! = (resid, x) -> resid .=  -0.1x)","category":"page"},{"location":"api/","page":"API","title":"API","text":"g! ::  FN  =  nothing in-place gradient function for 3 with mutable arrays as input. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"speedmapping([1.,1.]; g! = (grad, x) -> grad .=  4x.^3)","category":"page"},{"location":"api/","page":"API","title":"API","text":"m and g are versions of m! and g! with a scalar or StaticArray as input and output.","category":"page"},{"location":"api/","page":"API","title":"API","text":"using StaticArrays\nspeedmapping(1.; m = x -> 0.9x)\nspeedmapping(SA[1.,1.]; m = x -> 0.9x)\nspeedmapping(1.; g = x -> 4x^3)\nspeedmapping(SA[1.,1.]; g = x -> 4x.^3)","category":"page"},{"location":"api/#Other-important-keyword-arguments","page":"API","title":"Other important keyword arguments","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"algo :: Symbol = r! !==  nothing  ?  :aa  :  :acx determines the method used.","category":"page"},{"location":"api/","page":"API","title":"API","text":":acx can be used to solve problem 1 or 3.\n:aa can be used to solve 1 or 2. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"By default: algo = :acx, unless r! is used, indicating problem 2.","category":"page"},{"location":"api/","page":"API","title":"API","text":"f ::  FN  =  nothing computes an objective function. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"For 3, f will be used to initialize the learning rate better.\nFor  1 using AA, f will be used ensure monotonicity of the algorithm. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"lower, upper::  Union{Nothing, T}  =  nothing define bounds on parameters which can be used with any problems and any algorithm, and be scalars or arrays.","category":"page"},{"location":"api/","page":"API","title":"API","text":"speedmapping([1.,1.]; g!  =  (grad, x)  -> grad .= 4x.^3, lower = [-Inf,2.])","category":"page"},{"location":"api/#Other-keyword-arguments","page":"API","title":"Other keyword arguments","text":"","category":"section"},{"location":"api/#Affecting-both-**ACX**-and-**AA**","page":"API","title":"Affecting both ACX and AA","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"cache ::  Union{AcxCache, AaCache, Nothing}  =  nothing","category":"page"},{"location":"api/","page":"API","title":"API","text":"Pre-allocated memory for ACX or AA","category":"page"},{"location":"api/","page":"API","title":"API","text":"abstol::  AbstractFloat  =  1e-8","category":"page"},{"location":"api/","page":"API","title":"API","text":"The absolute tolerance used as stopping criterion. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"For 1, the algorithm stops when |xout - xin| < abstol (from m!(xout, xin))\nFor 2, the algorithm stops when |res| < abstol (from r!(res, x))\nFor 3, the algorithm stops when |grad| < abstol (from g!(grad, x))\npnorm ::  Real  =  2.","category":"page"},{"location":"api/","page":"API","title":"API","text":"The norm used for the stopping criterion. Typically 1, 2 or Inf.  ","category":"page"},{"location":"api/","page":"API","title":"API","text":"maps_limit :: Real  =  1_000_000_000","category":"page"},{"location":"api/","page":"API","title":"API","text":"The number of main function evaluation (m!, r!, or g!) before the algorithm terminates.","category":"page"},{"location":"api/","page":"API","title":"API","text":"iter_limit :: Real  =  1_000_000_000  ","category":"page"},{"location":"api/","page":"API","title":"API","text":"The number of iterations before the algorithm terminates.","category":"page"},{"location":"api/","page":"API","title":"API","text":"time_limit :: Real  =  Inf The time limit before stopping (if time_limit == Inf, time() will not be called at each iteration).\nreltol_resid_grow :: Real = algo == :aa ? 4. : (g! !== nothing || g !== nothing) ? 1e5 : 100","category":"page"},{"location":"api/","page":"API","title":"API","text":"reltol_resid_grow is a problem-specific stabilizing parameter. After a mapping/descent step/iteration, the distance between the current x and the previous x is reduced until the residual norm (|xout - xin|, |res|, or |grad|) does not increase more than a by a factor reltol_resid_grow. It is set to 4 for AA because this algorithm is more sensitive to low-quality iterations and because problem 2 may involve highly divergent functions. For ACX it is set to 100 for problem 1, and for 1e5 for problem 3.","category":"page"},{"location":"api/","page":"API","title":"API","text":"buffer::  AbstractFloat  = (m! !==  nothing  || m !==  nothing) ?  0.05  :  0.","category":"page"},{"location":"api/","page":"API","title":"API","text":"bufferis used in conjunction with lower or upper. If an iterate x lands outside a constraint, buffer leaves some distance between x and the constraint. It is set by default to 0.05 for 1 because constraints may be used to avoid landing immediately on bad values like saddle points at which the algorithm would stall. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"store_trace ::  Bool = false","category":"page"},{"location":"api/","page":"API","title":"API","text":"To store information on each iteration of the solving process. The trace depends on the algorithm. For a SpeedMapping result res, the trace is ","category":"page"},{"location":"api/","page":"API","title":"API","text":"res.acx_trace for ACX;\nres.aa_trace for AA.","category":"page"},{"location":"api/#Affecting-only-**ACX**","page":"API","title":"Affecting only ACX","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"orders = (2,3,3)","category":"page"},{"location":"api/","page":"API","title":"API","text":"The extrapolation orders. (2,3,3) is extremely reliable, but others like (2,3), or (2,) could be considered.","category":"page"},{"location":"api/","page":"API","title":"API","text":"initial_learning_rate ::  Real  =  1.","category":"page"},{"location":"api/","page":"API","title":"API","text":"The initial learning rate used for 3. If initialize_learning_rate == true, it is the starting point for the initialization.","category":"page"},{"location":"api/","page":"API","title":"API","text":"initializelearningrate ::  Bool  =  true","category":"page"},{"location":"api/","page":"API","title":"API","text":"To find a suitable learning rate to start 3 for which the residual norm does not increase too fast and the change in the objective (if supplied) respects the Armijo condition.","category":"page"},{"location":"api/#Affecting-only-**AA**","page":"API","title":"Affecting only AA","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"lags ::  Integer  =  30","category":"page"},{"location":"api/","page":"API","title":"API","text":"The maximum number of past residuals used to compute the next iterate","category":"page"},{"location":"api/","page":"API","title":"API","text":"condition_max :: Real = 1e6 The maximum condition number of the matrix of past residuals used to compute the next iterate. Setting it too high increases the risk of numerical imprecision.\nrel_default :: Real  =  1. The default relaxation parameter (also referred to as damping or mixing parameter).\nadarelax :: Symbol = m! !== nothing ? :minimum_distance : :none","category":"page"},{"location":"api/","page":"API","title":"API","text":"Adaptive relaxation. For now, only :minimum_distance is implemented (see Lepage-Saucier, 2024 although changes were made to the regularization). It is set to :none for problem 2 since :minimum_distance requires convergent mapping to be useful.","category":"page"},{"location":"api/","page":"API","title":"API","text":"composite :: Symbol = :none","category":"page"},{"location":"api/","page":"API","title":"API","text":"Composite Anderson Acceleration by Chen and Vuik, 2022. A one-step AA iteration (using 2 maps) is inserted between 2 full AA steps, which reduces  the computation and can offer interesting speed-up for some applications. Two types are implemented: :aa1 and acx2 (which inserts an ACX, order 2 step).","category":"page"},{"location":"api/","page":"API","title":"API","text":"abstol_obj_grow ::  Real  =  √abstol ","category":"page"},{"location":"api/","page":"API","title":"API","text":"If f is supplied,  AA monitors the growth of the objective. ","category":"page"},{"location":"api/#SpeedMappingResult","page":"API","title":"SpeedMappingResult","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SpeedMappingResult as fields","category":"page"},{"location":"api/","page":"API","title":"API","text":"minimizer  typeof(x0): The solution\nresidual_norm  AbstractFloat: The norm of the residual, which would be |xout - xin| for problem 1, |residual| for problem 2, and |∇f(x)| for problem 3 (only for non-binding components of the gradient).\nmaps: the number of maps, function evaluations or gradient evaluations\nf_calls: The number of objective function evaluations\niterations: The number of iteration\nstatus  Symbol: Should equal first_order if a solution has been found.\nalgo  (acx aa)\nacx_trace A vector of AcxState if algo == :acx && store_trace == true, nothing otherwise.\naa_trace A vector of AaState if algo == :aa && store_trace == true, nothing otherwise.\nlast_learning_rate  AbstractFloat The last learning rate, only meaningful for problem 3.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"CurrentModule = SpeedMapping","category":"page"},{"location":"#MyTestPkg","page":"Introduction","title":"MyTestPkg","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Documentation for SpeedMapping.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"EditURL = \"tutorial.jl\"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping(x0; kwargs...)  solves three types of problems:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Accelerating convergent mapping iterations\nSolving non-linear systems of equations\nMinimizing a function, possibly with box constraints","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using two algorithms:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Alternating cyclic extrapolations (ACX) Lepage-Saucier, 2024\nAnderson Acceleration (AA) Anderson, 1964","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This tutorial will display its main functionality on simple problems. To see which specification may be more performant for your problem, the Benchmarks section compares all of them, along with other Julia packages with similar functionalities.","category":"page"},{"location":"tutorial/#Accelerating-convergent-mapping-iterations","page":"Tutorial","title":"Accelerating convergent mapping iterations","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's find the dominant eigenvalue of a matrix A using the accelerated Power iteration.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra\n\nn = 10;\nA = ones(n,n) .+ Diagonal(1:n);\n\n# An in-place mapping function to avoid allocations\nfunction power_iteration!(xout, xin, A)\n    mul!(xout, A, xin)\n    maxabs = 0.\n    for xi in xout\n        abs(xi) > maxabs && (maxabs = abs(xi))\n    end\n    xout ./= maxabs\nend;\nx0 = ones(n);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Speedmapping has one mandatory argument: the starting point x0. The mapping is specified with the keyword argument m.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using SpeedMapping\nres = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A));\ndisplay(res)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The dominant eigenvalue is:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"v = res.minimizer; ## The dominant eigenvector\ndominant_eigenvalue = v'A*v/v'v;\neigen(A).values[10] ≈ dominant_eigenvalue","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With m!, the default algorithm is algo = :acx. To switch, set algo = :aa.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res_aa = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa);\ndisplay(res_aa)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default, AA uses adaptive relaxation, which can reduce the number of iterations. It is specified by the keyword argument adarelax = :minimum_distance. For constant relaxation, set adarelax = :none.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa, adarelax = :none);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Another recent development for AA is Composite AA by Chen and Vuik, 2022. A one-step AA iteration (using 2 maps) is inserted between 2 full AA steps, which reduces the computation and can speed-up some applications. The default is composite = :none. Two versions are available:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa, composite = :aa1);\nres = speedmapping(x0; m! = (xout, xin) -> power_iteration!(xout, xin, A), algo = :aa, composite = :acx2);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Some mapping iterations maximize or minimize a certain objective function. Since some AA steps can deteriorate the objective, it would be best to avoid them by falling back to the last map. This can be done by supplying an objective function (assumed to be a minimization problem) using f as keyword argument. Here is an illustrative EM-algorithm example from Hasselblad (1969).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function neg_log_likelihood(x)\n    freq = (162, 267, 271, 185, 111, 61, 27, 8, 3, 1)\n    p, μ1, μ2 = x\n    yfact = μ1expy = μ2expy = 1\n    log_lik = 0\n    for y in eachindex(freq)\n        log_lik += freq[y] * log((p * exp(-μ1) * μ1expy + (1 - p) * exp(-μ2) * μ2expy) / yfact)\n        yfact *= y\n        μ1expy *= μ1\n        μ2expy *= μ2\n    end\n    return -log_lik # Negative log likelihood to get a minimization problem\nend\n\nfunction EM_map!(xout, xin)\n    freq = (162, 267, 271, 185, 111, 61, 27, 8, 3, 1)\n    p, μ1, μ2 = xin\n    sum_freq_z1 = sum_freq_z2 = sum_freq_y_z1 = sum_freq_y_z2 = 0\n    μ1expy = μ2expy = 1\n    for i in eachindex(freq)\n        z = p * exp(-μ1) * μ1expy / (p * exp(-μ1) * μ1expy + (1 - p) * exp(-μ2) * μ2expy)\n        sum_freq_z1   += freq[i] * z\n        sum_freq_z2   += freq[i] * (1 - z)\n        sum_freq_y_z1 += (i - 1) * freq[i] * z\n        sum_freq_y_z2 += (i - 1) * freq[i] * (1 - z)\n        μ1expy *= μ1\n        μ2expy *= μ2\n    end\n    xout .= (sum_freq_z1 / sum(freq), sum_freq_y_z1 / sum_freq_z1, sum_freq_y_z2 / sum_freq_z2)\nend\n\nres_with_objective = speedmapping([0.25, 1., 2.]; f = neg_log_likelihood, m! = EM_map!, algo = :aa);\ndisplay(res_with_objective)","category":"page"},{"location":"tutorial/#Avoiding-memory-allocation","page":"Tutorial","title":"Avoiding memory allocation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For similar problems solved many times, it is possible to preallocate working memory and feed it using the cache keyword argument. Each algorithm has its own cache:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"acx_cache = AcxCache(x0);\naa_cache = AaCache(x0);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that x0 must still be supplied to speedmapping.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For small-sized problems with ACX, heap-allocation can be avoided by supplying a static array or a tuple as starting point and using the keyword argument m for the mapping function, offering additional speed gains.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using StaticArrays\n\nfunction power_iteration(xin, A)\n    xout = A * xin;\n    maxabs = 0.;\n    for xi in xout\n        abs(xi) > maxabs && (maxabs = abs(xi))\n    end;\n    return xout / maxabs;\nend;\n\nAs = @SMatrix ones(n,n);\nAs += Diagonal(1:n);\nx0s = @SVector ones(n);\n\nres_static = speedmapping(x0s; m = x -> power_iteration(x, As));\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Comparing speed gains","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using BenchmarkTools, Unitful\n\nbench_eigen = @benchmark eigen($A);\nbench_alloc = @benchmark speedmapping($x0; m! = (xout, xin) -> power_iteration!(xout, xin, $A));\nbench_prealloc = @benchmark speedmapping($x0; m! = (xout, xin) -> power_iteration!(xout, xin, $A), cache = $acx_cache);\nbench_nonalloc = @benchmark speedmapping($x0s; m = x -> power_iteration(x, $As));\ntimes = Int.(round.(median.([bench_eigen.times, bench_alloc.times, bench_prealloc.times, bench_nonalloc.times])))/1000 .* u\"μs\";\nreturn hcat([\"eigen\", \"Allocating\", \"Pre-allocated\", \"Non allocating\"],times)","category":"page"},{"location":"tutorial/#Working-with-scalars","page":"Tutorial","title":"Working with scalars","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m also accepts scalars and tuples.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping(0.5; m = cos);\nspeedmapping((0.5, 0.5); m = x -> (cos(x[1]), sin(x[2])));\nnothing #hide","category":"page"},{"location":"tutorial/#Solving-non-linear-systems-of-equations","page":"Tutorial","title":"Solving non linear systems of equations","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For non-linear systems of equations (finding x^* such that G(x^*) = 0), only AA with constant relaxation should be used (and is set by default). The keyword argument to supply G is r!.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function r!(resid, x)\n\tresid[1] = x[1]^2;\n\tresid[2] = (x[2] + x[1])^3;\nend\n\nspeedmapping([1.,2.]; r! = r!);\nnothing #hide","category":"page"},{"location":"tutorial/#Minimizing-a-function","page":"Tutorial","title":"Minimizing a function","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To minimize a function (using ACX), the function and its in-place gradient are supplied with the keyword arguments f and g!. The Hessian cannot be supplied. Compared to other quasi-Newton algorithms like L-BFGS, ACX iterations are very fast, but the algorithm may struggle for ill-conditioned problems.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"f_Rosenbrock(x) = 100 * (x[1]^2 - x[2])^2 + (x[1] - 1.)^2;\n\nfunction g_Rosenbrock!(grad, x) # Rosenbrock gradient\n\tgrad[1] = 400 * (x[1]^2 - x[2]) * x[1] + 2 * (x[1] - 1);\n\tgrad[2] = -200 * (x[1]^2 - x[2]);\nend\n\ndisplay(speedmapping([-1.2, 1.]; f = f_Rosenbrock, g! = g_Rosenbrock!))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The function objective is only used to compute a safer initial learning rate. It can be omitted.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping([-1.2, 1.]; g! = g_Rosenbrock!);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If only the objective is supplied, the gradient is computed using using ForwardDiff.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping([-1.2, 1.]; f = f_Rosenbrock);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The keyword argument g can be used with static arrays or tuple to supply a non-allocating gradient.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using StaticArrays\ng_Rosenbrock(x :: StaticArray) = SA[400 * (x[1]^2 - x[2]) * x[1] + 2 * (x[1] - 1), -200 * (x[1]^2 - x[2])];\nspeedmapping(SA[-1.2, 1.]; g = g_Rosenbrock);\n\ng_Rosenbrock(x :: Tuple) = (400 * (x[1]^2 - x[2]) * x[1] + 2 * (x[1] - 1), -200 * (x[1]^2 - x[2]));\nspeedmapping((-1.2, 1.); g = g_Rosenbrock);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Scalar functions can also be supplied. E.g. f(x) = e^x + x^2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res_scalar = speedmapping(0.; f = x -> exp(x) + x^2, g = x -> exp(x) + 2x);\ndisplay(res_scalar)","category":"page"},{"location":"tutorial/#Adding-box-constraint","page":"Tutorial","title":"Adding box constraint","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"An advantage of ACX is that constraints on parameters have little impact on estimation speed. They are added with the keyword arguments lower and upper (= nothing by default). The starting point does not need to be in the feasible domain, but, if supplied, upper / lower need to be of type x0.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"speedmapping([-1.2, 1.]; f = f_Rosenbrock, g! = g_Rosenbrock!, lower = [2, -Inf]);\nspeedmapping(0.; g = x -> exp(x) + 2x, upper = -1.);\nnothing #hide","category":"page"}]
}
